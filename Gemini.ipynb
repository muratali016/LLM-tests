{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_JPbRK7l-sU"
      },
      "outputs": [],
      "source": [
        "google_key=\"X\" #\n",
        "openaikey=\"x\"\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"]=google_key\n",
        "os.environ[\"OPENAI_API_KEY\"]=openaikey\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"X\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu #gpu\n",
        "!pip install langchain_openai\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "QdFSeOvInH2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text=\"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader= PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text+= page.extract_text()\n",
        "    return  text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    embeddings = OpenAIEmbeddings( )\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    return vector_store\n",
        "\n",
        "pdf_docs = [\"/content/resnet.pdf\",\"/content/unet.pdf\"]\n",
        "\n",
        "texts=get_pdf_text(pdf_docs)\n",
        "chunks=get_text_chunks(texts)\n",
        "vector_store = FAISS.from_texts(chunks, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "pMuD7pduKOBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity search within the vector database"
      ],
      "metadata": {
        "id": "29inEekzun_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"best resnet models\"\n",
        "docs = vector_store.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "D9dbRcCmRafc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini Pro"
      ],
      "metadata": {
        "id": "RFrRvPRouX2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import HuggingFaceHub\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             convert_system_message_to_human=True)\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "memory_key='chat_history', return_messages=True)\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.7}),\n",
        "        memory=memory\n",
        "    )\n",
        "response = conversation_chain.invoke(\"what is resnet tell me about it\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgzXvZESKOHG",
        "outputId": "cce14f7c-d9f4-402f-b4c0-2d69f694eda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'what is resnet tell me about it',\n",
              " 'chat_history': [HumanMessage(content='what is resnet tell me about it'),\n",
              "  AIMessage(content='ResNet, which stands for Residual Neural Network, is a type of deep neural network that was introduced in 2015 by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ResNets are designed to address the problem of vanishing gradients, which can occur in deep neural networks and make it difficult to train them.\\n\\nResNets are constructed by stacking residual blocks, which are composed of multiple convolutional layers and a shortcut connection. The shortcut connection allows the gradients to flow directly from the input of the residual block to the output, which helps to prevent the gradients from vanishing.\\n\\nResNets have achieved state-of-the-art results on a variety of image classification tasks, including the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). In 2015, a ResNet won the ILSVRC competition with a top-5 error rate of 3.57%, which was a significant improvement over the previous state-of-the-art.\\n\\nResNets have also been used successfully for other tasks, such as object detection, semantic segmentation, and natural language processing. They are now one of the most widely used types of deep neural networks.\\n\\nHere are some of the advantages of ResNets:\\n\\n* They are easier to train than traditional deep neural networks.\\n* They can achieve higher accuracy on a variety of tasks.\\n* They are relatively efficient in terms of computational cost.\\n\\nHere are some of the disadvantages of ResNets:\\n\\n* They can be more complex to design and implement than traditional deep neural networks.\\n* They can be more computationally expensive than traditional deep neural networks.\\n\\nOverall, ResNets are a powerful type of deep neural network that have achieved state-of-the-art results on a variety of tasks. They are relatively easy to train and can achieve high accuracy. However, they can be more complex and computationally expensive than traditional deep neural networks.')],\n",
              " 'answer': 'ResNet, which stands for Residual Neural Network, is a type of deep neural network that was introduced in 2015 by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ResNets are designed to address the problem of vanishing gradients, which can occur in deep neural networks and make it difficult to train them.\\n\\nResNets are constructed by stacking residual blocks, which are composed of multiple convolutional layers and a shortcut connection. The shortcut connection allows the gradients to flow directly from the input of the residual block to the output, which helps to prevent the gradients from vanishing.\\n\\nResNets have achieved state-of-the-art results on a variety of image classification tasks, including the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). In 2015, a ResNet won the ILSVRC competition with a top-5 error rate of 3.57%, which was a significant improvement over the previous state-of-the-art.\\n\\nResNets have also been used successfully for other tasks, such as object detection, semantic segmentation, and natural language processing. They are now one of the most widely used types of deep neural networks.\\n\\nHere are some of the advantages of ResNets:\\n\\n* They are easier to train than traditional deep neural networks.\\n* They can achieve higher accuracy on a variety of tasks.\\n* They are relatively efficient in terms of computational cost.\\n\\nHere are some of the disadvantages of ResNets:\\n\\n* They can be more complex to design and implement than traditional deep neural networks.\\n* They can be more computationally expensive than traditional deep neural networks.\\n\\nOverall, ResNets are a powerful type of deep neural network that have achieved state-of-the-art results on a variety of tasks. They are relatively easy to train and can achieve high accuracy. However, they can be more complex and computationally expensive than traditional deep neural networks.'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"answer\"]"
      ],
      "metadata": {
        "id": "BOfMb1dWUoLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-4 0125-preview"
      ],
      "metadata": {
        "id": "Uh2n-_UHuZsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import HuggingFaceHub\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "memory_key='chat_history', return_messages=True)\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.7}),\n",
        "        memory=memory\n",
        "    )\n",
        "response = conversation_chain.invoke(\"what is resnet tell me about it\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f94ptlBMudU4",
        "outputId": "36598e8a-644f-4b3a-c3b7-03290ab506d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'what is resnet tell me about it',\n",
              " 'chat_history': [HumanMessage(content='what is resnet tell me about it'),\n",
              "  AIMessage(content='ResNet, short for Residual Network, is a type of convolutional neural network (CNN) architecture designed to enable the training of much deeper networks. It was introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun from Microsoft Research in the paper \"Deep Residual Learning for Image Recognition\". The key innovation of ResNet is its use of residual learning blocks that make it easier to train very deep networks.\\n\\n### Key Features and Innovations:\\n\\n1. **Residual Blocks**: The core idea of ResNet is introducing a so-called \"shortcut connection\" that skips one or more layers. Traditional neural networks try to directly learn the desired underlying mapping from inputs to outputs. In contrast, ResNet reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.\\n\\n2. **Ease of Optimization**: By using residual blocks, ResNet addresses the degradation problem where deeper networks start performing worse than shallower ones not because of overfitting but due to the increased complexity that makes the network harder to train. With residual blocks, deeper networks can be trained by effectively learning the identity function as the additional layers, making it easier to add layers without losing performance.\\n\\n3. **Deeper Networks**: The introduction of ResNet allowed for the development of networks that are much deeper than previously possible. The original paper presented models with depths of up to 152 layers, significantly deeper than the VGG and GoogLeNet models that were state-of-the-art at the time. Despite its depth, a 152-layer ResNet has lower complexity than VGG nets.\\n\\n4. **Improved Performance**: ResNet models have shown remarkable performance on several benchmarks. For instance, on the ImageNet dataset, a 152-layer ResNet won the 1st place on the ILSVRC 2015 classification task, demonstrating a significant improvement over previous architectures. It also achieved excellent results in other tasks beyond classification, such as object detection and localization.\\n\\n5. **Generalization to Other Tasks**: The principles of residual learning are not limited to image classification and have been successfully applied to a wide range of other vision and non-vision tasks.\\n\\n### Impact and Legacy:\\n\\nThe introduction of ResNet has had a profound impact on the field of deep learning. It has enabled the training of networks that are significantly deeper than was previously possible, leading to improvements in accuracy and performance across a wide range of tasks. ResNet has inspired many subsequent research efforts and architectures, cementing its place as a foundational model in deep learning. The concept of residual learning and shortcut connections has influenced the design of many neural network architectures that followed.')],\n",
              " 'answer': 'ResNet, short for Residual Network, is a type of convolutional neural network (CNN) architecture designed to enable the training of much deeper networks. It was introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun from Microsoft Research in the paper \"Deep Residual Learning for Image Recognition\". The key innovation of ResNet is its use of residual learning blocks that make it easier to train very deep networks.\\n\\n### Key Features and Innovations:\\n\\n1. **Residual Blocks**: The core idea of ResNet is introducing a so-called \"shortcut connection\" that skips one or more layers. Traditional neural networks try to directly learn the desired underlying mapping from inputs to outputs. In contrast, ResNet reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.\\n\\n2. **Ease of Optimization**: By using residual blocks, ResNet addresses the degradation problem where deeper networks start performing worse than shallower ones not because of overfitting but due to the increased complexity that makes the network harder to train. With residual blocks, deeper networks can be trained by effectively learning the identity function as the additional layers, making it easier to add layers without losing performance.\\n\\n3. **Deeper Networks**: The introduction of ResNet allowed for the development of networks that are much deeper than previously possible. The original paper presented models with depths of up to 152 layers, significantly deeper than the VGG and GoogLeNet models that were state-of-the-art at the time. Despite its depth, a 152-layer ResNet has lower complexity than VGG nets.\\n\\n4. **Improved Performance**: ResNet models have shown remarkable performance on several benchmarks. For instance, on the ImageNet dataset, a 152-layer ResNet won the 1st place on the ILSVRC 2015 classification task, demonstrating a significant improvement over previous architectures. It also achieved excellent results in other tasks beyond classification, such as object detection and localization.\\n\\n5. **Generalization to Other Tasks**: The principles of residual learning are not limited to image classification and have been successfully applied to a wide range of other vision and non-vision tasks.\\n\\n### Impact and Legacy:\\n\\nThe introduction of ResNet has had a profound impact on the field of deep learning. It has enabled the training of networks that are significantly deeper than was previously possible, leading to improvements in accuracy and performance across a wide range of tasks. ResNet has inspired many subsequent research efforts and architectures, cementing its place as a foundational model in deep learning. The concept of residual learning and shortcut connections has influenced the design of many neural network architectures that followed.'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}